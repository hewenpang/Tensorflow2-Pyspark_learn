{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##自定义Attention层\n",
    "class attention(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    his_len:用户序列长度\n",
    "    (item_dim+cate_dim):embedding长度\n",
    "    '''\n",
    "    def __init__(self, keys_dim, dim_layers):#keys_dim 代表key的数量，dim_layers定义全链接层的维度列表\n",
    "        super(attention,self).__init__()\n",
    "        self.keys_dim = keys_dim\n",
    "\n",
    "        self.fc = tf.keras.Sequential()\n",
    "        for dim_layer in dim_layers[:-1]:\n",
    "            self.fc.add(layers.Dense(dim_layer,activation='sigmoid'))\n",
    "        self.fc.add(layers.Dense(dim_layers[-1],activation=None))\n",
    "\n",
    "    def call(self,queries,keys,keys_length):\n",
    "        # 将 queries 复制多次，复制次数与序列长度相同\n",
    "        # tf.shape(keys)[1] 即序列长度\n",
    "        # 得到(batch, his_len, (item_dim+cate_dim))，his_len为序列长度\n",
    "        queries = tf.tile(tf.expand_dims(queries, 1), [1, tf.shape(keys)[1], 1])##最终变为[batch_size, tf.shape(keys)[1], query_dim]\n",
    "        # outer product ?\n",
    "        # *运算等价于 tf.multiply，是元素级别的相乘，对应位置相乘。 而 tf.matmul 则是矩阵乘法\n",
    "        # (batch, his_len, (item_dim+cate_dim) * 4)\n",
    "        din_all = tf.concat([queries, keys, queries-keys, queries*keys], axis=-1)\n",
    "        # (batch, his_len, 1) => (batch, 1, his_len)\n",
    "        outputs = tf.transpose(self.fc(din_all), [0,2,1])\n",
    "\n",
    "        # Mask, 返回shape为(batch, his_len)的布尔矩阵\n",
    "        key_masks = tf.sequence_mask(keys_length, max(keys_length), dtype=tf.bool) \n",
    "        # 扩展一个维度，使得 mask 与 outputs 维度匹配：(batch, 1, his_len)\n",
    "        key_masks = tf.expand_dims(key_masks, 1) \n",
    "        # 构造一个与outputs形状相同的由近似等于0的数值(-2 ** 32 + 1)构成的矩阵\n",
    "        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1) \n",
    "        # padding的mask后补一个很小的负数，这样softmax之后就会接近0.\n",
    "        # 从outputs中取出key_masks为True对应的部分，而False对应的部分用极小的数(-2 ** 32 + 1) 代替\n",
    "        # outputs 最终维度仍然是 (batch, 1, his_len)\n",
    "        outputs = tf.where(key_masks, outputs, paddings)#如果key_masks对应位置为true则取outputs的对应位置元素，反之则取paddings的对应位置元素\n",
    "\n",
    "        # Scale，除以嵌入维度开平方后的数值，有利于训练\n",
    "        outputs = outputs / (self.keys_dim ** 0.5)\n",
    "\n",
    "        # Activation\n",
    "        outputs = tf.keras.activations.softmax(outputs)##outputs 为attention计算出来的权重\n",
    "\n",
    "\n",
    "        # 加权求和\n",
    "        # keys: (batch, his_len, item_dim+cate_dim)\n",
    "        # (batch, 1, his_len) x (batch, his_len, item_dim+cate_dim)\n",
    "        # 得到：(batch, 1, item_dim+cate_dim) =>  (batch, item_dim+cate_dim)\n",
    "        outputs = tf.squeeze(tf.matmul(outputs, keys))\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "#自适应激活函数\n",
    "class dice(tf.keras.layers.Layer):\n",
    "    def __init__(self, feat_dim):\n",
    "        super(dice, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.alphas= tf.Variable(tf.zeros([feat_dim]), dtype=tf.float32)\n",
    "        self.beta  = tf.Variable(tf.zeros([feat_dim]), dtype=tf.float32)\n",
    "\n",
    "        # center=False, scale=False 目的是仅对batch数据标准化，得到 z_norm\n",
    "        # 而不进行额外的 γ*z_norm + β 的操作\n",
    "        self.bn = tf.keras.layers.BatchNormalization(center=False, scale=False)\n",
    "\n",
    "    def call(self, _x, axis=-1, epsilon=0.000000001):\n",
    "\n",
    "        # reduction_axes = list(range(len(_x.get_shape())))\n",
    "        # del reduction_axes[axis]\n",
    "        # broadcast_shape = [1] * len(_x.get_shape())\n",
    "        # broadcast_shape[axis] = self.feat_dim\n",
    "\n",
    "        # mean = tf.reduce_mean(_x, axis=reduction_axes)\n",
    "        # brodcast_mean = tf.reshape(mean, broadcast_shape)\n",
    "        # std = tf.reduce_mean(tf.square(_x - brodcast_mean) + epsilon, axis=reduction_axes)\n",
    "        # std = tf.sqrt(std)\n",
    "        # brodcast_std = tf.reshape(std, broadcast_shape)\n",
    "\n",
    "        # 根据论文中的描述：标准化后使用 sigmoid 函数得到 x_p\n",
    "        x_normed = self.bn(_x)\n",
    "        x_p = tf.keras.activations.sigmoid(self.beta * x_normed)\n",
    "        # 根据论文公式计算激活函数的输出值\n",
    "        return self.alphas * (1.0 - x_p) * _x + x_p * _x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
